{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm \n",
    "from time import sleep\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for debugging\n",
    "def dprint(s, debug):\n",
    "    if debug:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Check Device ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please log in to your Hugging Face account.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0378fafd0baa4c1595d974e7e923b3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    }
   ],
   "source": [
    "# --- Model Loading ---\n",
    "# We'll run our models using Hugging Face's transformers library on HPC/ Google Colab/ Lightning.ai\n",
    "# The Llama models are gated, meaning you must request access on their Hugging Face pages.\n",
    "# Once you have access, you need to log in here to download the model weights.\n",
    "\n",
    "# Run this command in your terminal when you are running this notebook for the 1st time\n",
    "# git config --global credential.helper store\n",
    "\n",
    "print(\"Please log in to your Hugging Face account.\")\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be our primary model for most of the assignment.\n",
    "model_id_1 = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer for meta-llama/Llama-2-7b-chat-hf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: meta-llama/Llama-2-7b-chat-hf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c748b8c249246aaa47f3c7446e205ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading tokenizer for {model_id_1}...\")\n",
    "# The tokenizer turns our text prompt into numbers the model can understand.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id_1)\n",
    "\n",
    "print(f\"Loading model: {model_id_1}...\")\n",
    "# This downloads the model weights to your environment.\n",
    "# torch_dtype=torch.bfloat16 uses half-precision floats to save memory.\n",
    "# device_map=\"auto\" automatically puts the model on the GPU if available.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_1,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"{model_id_1} model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(prompt, student_configs, post_processing_fn, model_obj, tokenizer_obj, debug=False):\n",
    "    \"\"\"\n",
    "    Generates a response using the provided local Hugging Face model and tokenizer.\n",
    "    \"\"\"\n",
    "    # 1. Tokenize the input prompt\n",
    "    inputs = tokenizer_obj(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    hf_configs = student_configs.copy()\n",
    "    if 'max_tokens' in hf_configs:\n",
    "        # `generate` uses `max_new_tokens` to specify the length of the output\n",
    "        hf_configs['max_new_tokens'] = hf_configs.pop('max_tokens')\n",
    "    if 'stop' in hf_configs:\n",
    "        del hf_configs['stop'] # Stop sequences are handled differently; we'll ignore for simplicity\n",
    "\n",
    "    # 2. Generate output tokens\n",
    "    outputs = model_obj.generate(**inputs, **hf_configs).to(device)\n",
    "    \n",
    "    # 3. Decode the generated tokens back to a string\n",
    "    # We slice the output to only get the newly generated text, not the original prompt\n",
    "    result_new = tokenizer_obj.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    dprint(\"************ Prompt ************\", debug)\n",
    "    dprint(prompt, debug)\n",
    "    dprint(\"\\n************ Raw Response ************\", debug)\n",
    "    dprint(result_new, debug)\n",
    "\n",
    "    # 4. Apply post-processing to extract the final answer\n",
    "    final_output = post_processing_fn(result_new)\n",
    "    \n",
    "    dprint(\"\\n************ Final Output ************\", debug)\n",
    "    dprint(final_output, debug)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_addition_pairs(lower_bound, upper_bound, rng):\n",
    "    \"\"\"Generates two random integers within a specified range.\"\"\"\n",
    "    int_a = int(np.ceil(rng.uniform(lower_bound, upper_bound)))\n",
    "    int_b = int(np.ceil(rng.uniform(lower_bound, upper_bound)))\n",
    "    return int_a, int_b\n",
    "\n",
    "def test_range(added_prompt, prompt_configs, rng, model_obj, tokenizer_obj, n_sample=30,\n",
    "               lower_bound=1, upper_bound=10, fixed_pairs=None,\n",
    "               pre_processing=lambda x:x, post_processing=lambda y:y,\n",
    "               debug=False):\n",
    "    \"\"\"\n",
    "    Tests a language model's addition performance over a range of numbers.\n",
    "\n",
    "    Args:\n",
    "        added_prompt (tuple): A tuple containing the prefix and suffix for the prompt.\n",
    "        prompt_configs (dict): Configuration parameters for the model's generate function.\n",
    "        rng (numpy.random.Generator): A random number generator instance.\n",
    "        model_obj (transformers.PreTrainedModel): The loaded Hugging Face model object.\n",
    "        tokenizer_obj (transformers.PreTrainedTokenizer): The loaded Hugging Face tokenizer object.\n",
    "        n_sample (int): The number of random pairs to generate if fixed_pairs is None.\n",
    "        lower_bound (int): The lower bound for number generation.\n",
    "        upper_bound (int): The upper bound for number generation.\n",
    "        fixed_pairs (list, optional): A list of specific integer tuples to test.\n",
    "        pre_processing (function): A function to apply to the input string before prompting.\n",
    "        post_processing (function): A function to extract the integer answer from the model's output.\n",
    "        debug (bool): If True, prints detailed debugging information.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing performance metrics (res, acc, mae, prompt_length).\n",
    "    \"\"\"\n",
    "    # --- Lists for storing results ---\n",
    "    int_as = []\n",
    "    int_bs = []\n",
    "    answers = []\n",
    "    model_responses = []\n",
    "    correct = []\n",
    "    prompts = []\n",
    "    \n",
    "    # --- Determine the test cases ---\n",
    "    iterations = range(n_sample) if fixed_pairs is None else fixed_pairs\n",
    "    \n",
    "    for v in tqdm(iterations):\n",
    "        if fixed_pairs is None:\n",
    "            # Generate two new numbers if no fixed pairs are provided\n",
    "            int_a, int_b = get_addition_pairs(lower_bound=lower_bound, upper_bound=upper_bound, rng=rng)\n",
    "        else:\n",
    "            # Use the provided fixed pairs\n",
    "            int_a, int_b = v\n",
    "            \n",
    "        # --- Construct the prompt for two numbers ---\n",
    "        fixed_prompt = f'{int_a}+{int_b}'\n",
    "        fixed_prompt = pre_processing(fixed_prompt)\n",
    "        \n",
    "        prefix, suffix = added_prompt\n",
    "        prompt = prefix + fixed_prompt + suffix\n",
    "        \n",
    "        # --- Get the model's response ---\n",
    "        model_response = call_model(prompt, prompt_configs, post_processing, model_obj, tokenizer_obj, debug=debug)\n",
    "        \n",
    "        # --- Calculate the correct answer for two numbers ---\n",
    "        answer = int_a + int_b\n",
    "        \n",
    "        # --- Append all results for analysis ---\n",
    "        int_as.append(int_a)\n",
    "        int_bs.append(int_b)\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "        model_responses.append(model_response)\n",
    "        correct.append((answer == model_response))\n",
    "        sleep(0.1)\n",
    "\n",
    "    # --- Create a DataFrame to display the results for two numbers ---\n",
    "    df = pd.DataFrame({\n",
    "        'int_a': int_as, \n",
    "        'int_b': int_bs, \n",
    "        'prompt': prompts, \n",
    "        'answer': answers, \n",
    "        'response': model_responses, \n",
    "        'correct': correct\n",
    "    })\n",
    "    print(df)\n",
    "    \n",
    "    # --- Calculate and return performance metrics ---\n",
    "    mae = mean_absolute_error(df['answer'], df['response'])\n",
    "    acc = df.correct.sum() / len(df)\n",
    "    prompt_length = len(prefix) + len(suffix)\n",
    "    res = acc * (1 / prompt_length) * (1 - mae / (1 * 10**4))\n",
    "    \n",
    "    return {'res': res, 'acc': acc, 'mae': mae, 'prompt_length': prompt_length}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part 1. Zero Shot Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Zero-shot single-digit addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19cecd6f6d543fe85315f5a5b1570b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   int_a  int_b                             prompt  answer  response  correct\n",
      "0      7      4   Question: What is 7+4?\\nAnswer:       11        11     True\n",
      "1      2      2   Question: What is 2+2?\\nAnswer:        4         5    False\n",
      "2      9     10  Question: What is 9+10?\\nAnswer:       19        19     True\n",
      "3      7      8   Question: What is 7+8?\\nAnswer:       15        15     True\n",
      "4      6     10  Question: What is 6+10?\\nAnswer:       16        16     True\n",
      "5      9      2   Question: What is 9+2?\\nAnswer:       11        11     True\n",
      "6      9      2   Question: What is 9+2?\\nAnswer:       11        11     True\n",
      "7      8      3   Question: What is 8+3?\\nAnswer:       11        11     True\n",
      "8      9      6   Question: What is 9+6?\\nAnswer:       15        15     True\n",
      "9      4      5   Question: What is 4+5?\\nAnswer:        9         9     True\n",
      "{'res': 0.031034172413793103, 'acc': 0.9, 'mae': 0.1, 'prompt_length': 29}\n"
     ]
    }
   ],
   "source": [
    "# All of this remains the same\n",
    "import re\n",
    "\n",
    "added_prompt = ('Question: What is ', '?\\\\nAnswer: ')\n",
    "prompt_config = {'max_tokens': 2,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "\n",
    "def your_pre_processing(input_string):\n",
    "    return input_string\n",
    "\n",
    "def your_post_processing(output_string):\n",
    "    only_digits = re.sub(r\"\\D\", \"\", output_string)\n",
    "    try:\n",
    "        res = int(only_digits)\n",
    "    except:\n",
    "        res = 0\n",
    "    return res\n",
    "\n",
    "# The model name string is no longer passed to the function\n",
    "# It was used in the previous cell to load the 'model' and 'tokenizer' objects\n",
    "print(f\"Testing model: {model_id_1}\")\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# This is the only line that changes\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt,\n",
    "    prompt_configs=prompt_config,\n",
    "    rng=rng,\n",
    "    model_obj=model, \n",
    "    tokenizer_obj=tokenizer,\n",
    "    n_sample=10,\n",
    "    lower_bound=1,\n",
    "    upper_bound=10,\n",
    "    fixed_pairs=None,\n",
    "    pre_processing=your_pre_processing,\n",
    "    post_processing=your_post_processing,\n",
    "    debug=False\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Zero-shot 7-digit addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3fee3498ca415199093b3206a81e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                        prompt    answer  \\\n",
      "0  6732655  3428081  Question: What is 6732655+3428081?\\nAnswer:   10160736   \n",
      "1  1368762  1148749  Question: What is 1368762+1148749?\\nAnswer:    2517511   \n",
      "2  8319432  9214800  Question: What is 8319432+9214800?\\nAnswer:   17534232   \n",
      "3  6459722  7565469  Question: What is 6459722+7565469?\\nAnswer:   14025191   \n",
      "4  5892625  9415651  Question: What is 5892625+9415651?\\nAnswer:   15308276   \n",
      "5  8342682  1024647  Question: What is 8342682+1024647?\\nAnswer:    9367329   \n",
      "6  8716638  1302271  Question: What is 8716638+1302271?\\nAnswer:   10018909   \n",
      "7  7566899  2580901  Question: What is 7566899+2580901?\\nAnswer:   10147800   \n",
      "8  8768610  5873151  Question: What is 8768610+5873151?\\nAnswer:   14641761   \n",
      "9  3697407  4804185  Question: What is 3697407+4804185?\\nAnswer:    8501592   \n",
      "\n",
      "   response  correct  \n",
      "0  10155436    False  \n",
      "1   2517511     True  \n",
      "2  17530200    False  \n",
      "3  14027388    False  \n",
      "4   5892625    False  \n",
      "5   9367109    False  \n",
      "6   9999999    False  \n",
      "7   9947800    False  \n",
      "8  14639761    False  \n",
      "9   8501562    False  \n",
      "{'res': -0.32925310344827585, 'acc': 0.1, 'mae': 964834.0, 'prompt_length': 29}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 8\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# The call to test_range is updated to pass the model and tokenizer objects.\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1a.** In your opinion, what are some factors that cause language model performance to deteriorate from 1 digit to 7 digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: In zero-shot addition, performance deteriorates from 1-digit to 7-digits addition because the model optimizes for next-token prediction, not algorithmic computation. For 1 digit addition, the model recalls memorized patterns such as 7 + 4 = 11. However, 7 digits addition requires step-wise reasoning, adding digits right to left, carrying over values. Short addition patterns are common in training dataset, but long 7-digits addition examples are rare. Without examples, the model approximates rather than implement arithmetic logic, and prediction errors keep adding up. Randomness from parameters such as temperature, top-k, or top-p also distort actual results. If the model outputs extra text or spaces, truncation and parsing error can occur during post-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1b**. Play around with the config parameters ('max_tokens','temperature','top_k','top_p','repetition_penalty')\n",
    "* What does each parameter represent?\n",
    "* How does increasing each parameter change the generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Temperature scales the model’s output logits before the softmax operation. A low temperature reduces stochasticity, and ensures identical results for the same input. A higher temperature introduces diversity which is decreases arithmetic precision because it won’t generate definite answer. \n",
    "Max Tokens: sets the maximum number of tokens the model can generate in one response. If max token is too small, the model’s output may be truncated. If max token is too large, the model keeps rambling on and produces redundant text even after producing correct result.\n",
    "Top-P: sampling retains the smallest set of tokens whose cumulative probability exceeds a chosen threshold p and the model samples only from this subset.Higher top-p values include low-probability tokens, increases diversity and error rates in addition outputs. In arithmetic, the correct digit typically has the highest probability, so a small top-p value is necessary. \n",
    "Top-K: At each decoding step, top-k restricts sampling to the k most probable tokens. Smaller k focuses the model on the most likely digits, increasing accuracy. Larger k allows random tokens to appear which results in wrong outputs and lower accuracy.\n",
    "Repetition Penalty: reduces the probability of tokens that have already appeared in the output sequence. Moderate penalty prevents repetition, however, excessive penalty distorts legitimate repeated digits such as 1122111."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1c. Do 7-digit addition with Qwen3 8B.\n",
    "\n",
    "* How does the performance change?\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 model offloaded and GPU memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# --- Before loading Qwen 3, offload Llama 2 to free up VRAM ---\n",
    "\n",
    "# 1. Delete the model and tokenizer variables from memory.\n",
    "# Replace 'model' and 'tokenizer' with the actual variable names you used for Llama 2.\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "# 2. Run Python's garbage collector and empty PyTorch's CUDA cache.\n",
    "# This is the crucial step to actually release the GPU memory.\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Llama 2 model offloaded and GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer for Qwen/Qwen3-8B...\n",
      "Loading model: Qwen/Qwen3-8B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c657c44db0549d1ba52f0edb7a3aebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen/Qwen3-8B model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Load Qwen 3 8B ---\n",
    "# This is a different model, so we need to load its specific tokenizer and weights.\n",
    "model_id_2 = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "print(f\"\\nLoading tokenizer for {model_id_2}...\")\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(model_id_2)\n",
    "\n",
    "print(f\"Loading model: {model_id_2}...\")\n",
    "model_2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_2,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"{model_id_2} model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_tokens': 20, 'temperature': 0.7, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1, 'stop': []}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb61e779c89640c0bb9787aa4521705e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                        prompt    answer  \\\n",
      "0  6732655  3428081  Question: What is 6732655+3428081?\\nAnswer:   10160736   \n",
      "1  1368762  1148749  Question: What is 1368762+1148749?\\nAnswer:    2517511   \n",
      "2  8319432  9214800  Question: What is 8319432+9214800?\\nAnswer:   17534232   \n",
      "3  6459722  7565469  Question: What is 6459722+7565469?\\nAnswer:   14025191   \n",
      "4  5892625  9415651  Question: What is 5892625+9415651?\\nAnswer:   15308276   \n",
      "5  8342682  1024647  Question: What is 8342682+1024647?\\nAnswer:    9367329   \n",
      "6  8716638  1302271  Question: What is 8716638+1302271?\\nAnswer:   10018909   \n",
      "7  7566899  2580901  Question: What is 7566899+2580901?\\nAnswer:   10147800   \n",
      "8  8768610  5873151  Question: What is 8768610+5873151?\\nAnswer:   14641761   \n",
      "9  3697407  4804185  Question: What is 3697407+4804185?\\nAnswer:    8501592   \n",
      "\n",
      "         response  correct  \n",
      "0        10160736     True  \n",
      "1  25175112517511    False  \n",
      "2        17534232     True  \n",
      "3        14025191     True  \n",
      "4        15308276     True  \n",
      "5         9367329     True  \n",
      "6  10018909871663    False  \n",
      "7        10147800     True  \n",
      "8        14641761     True  \n",
      "9  84999923697407    False  \n",
      "{'res': -29012326.711723555, 'acc': 0.7, 'mae': 12019392504856.9, 'prompt_length': 29}\n"
     ]
    }
   ],
   "source": [
    "# --- Test on 7-digit addition ---\n",
    "prompt_config['max_tokens'] = 20\n",
    "\n",
    "print(prompt_config)\n",
    "rng = np.random.default_rng(seed)\n",
    "res_2 = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model_2,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer_2,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: When running zero-shot 7-digits addition, Qwen-3B-8 achieved an accuracy of 0.9 and MAE of 200, indicating that 9 out of 10 predictions were correct and the single incorrect result was almost close to the true sum. It shows higher precision and generalization to 7-digits addition.\n",
    "\n",
    "Qwen handles multi-digit arithmetic and exhibits lower numerical errors in zero-shot settings. It is trained on larger and diverse datasets containing structured numeric and symbolic text, instruction fine-tuned for factual accuracy. This allows the model to maintain carry propagation more effectively without explicit prompts and examples. Qwen’s decoding is deterministic under same parameter settings temperature, top-p, top-k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1d.** Here we're giving our language model the prior that the sum of two 7-digit numbers must have a maximum of 8 digits. (by setting max_token=8). What if we remove this prior by increasing the max_token to 20? \n",
    "* Does the model perform well?\n",
    "* What are some reasons why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: When max tokens is increased from 8 to 20, the model’s performance degraded, accuracy dropped from 0.9 to 0.6, and MAE increased drastically. The model keeps rambling on and generates longer texts even after giving the correct answer. It appends extra numbers and repeats digits which corrupts post-processing and returns error values. The model didn’t produce numerically valid answers once the output allowed for more tokens.\n",
    "\n",
    "More tokens increase the number of prediction steps, and small token-level errors keep adding up. Randomness in decoding with temperature, top-p, and top-k generates multiple incorrect digits and distort the actual sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e03c22d1394ab19b2fa10a3c665145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                        prompt    answer  \\\n",
      "0  6732655  3428081  Question: What is 6732655+3428081?\\nAnswer:   10160736   \n",
      "1  1368762  1148749  Question: What is 1368762+1148749?\\nAnswer:    2517511   \n",
      "2  8319432  9214800  Question: What is 8319432+9214800?\\nAnswer:   17534232   \n",
      "3  6459722  7565469  Question: What is 6459722+7565469?\\nAnswer:   14025191   \n",
      "4  5892625  9415651  Question: What is 5892625+9415651?\\nAnswer:   15308276   \n",
      "5  8342682  1024647  Question: What is 8342682+1024647?\\nAnswer:    9367329   \n",
      "6  8716638  1302271  Question: What is 8716638+1302271?\\nAnswer:   10018909   \n",
      "7  7566899  2580901  Question: What is 7566899+2580901?\\nAnswer:   10147800   \n",
      "8  8768610  5873151  Question: What is 8768610+5873151?\\nAnswer:   14641761   \n",
      "9  3697407  4804185  Question: What is 3697407+4804185?\\nAnswer:    8501592   \n",
      "\n",
      "         response  correct  \n",
      "0        10160736     True  \n",
      "1         2517511     True  \n",
      "2        17534232     True  \n",
      "3  14025191769834    False  \n",
      "4  15308276498293    False  \n",
      "5         9367329     True  \n",
      "6  87166381302271    False  \n",
      "7        10147800     True  \n",
      "8        14641761     True  \n",
      "9         8499592    False  \n",
      "{'res': -24103408.990349375, 'acc': 0.6, 'mae': 11649981022002.2, 'prompt_length': 29}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 20\n",
    "rng = np.random.default_rng(seed)\n",
    "res_2 = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model_2,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer_2,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen/Qwen3-8B offloaded and GPU memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# 1. Delete the model and tokenizer variables from memory.\n",
    "# Replace 'model' and 'tokenizer' with the actual variable names you used for Llama 2.\n",
    "del model_2\n",
    "del tokenizer_2\n",
    "\n",
    "# 2. Run Python's garbage collector and empty PyTorch's CUDA cache.\n",
    "# This is the crucial step to actually release the GPU memory.\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"{model_id_2} offloaded and GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. In Context Learning\n",
    "\n",
    "We will try to improve the performance of 7-digit addition via in-context learning.\n",
    "We will use [llama-2-7b]. Below is a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer for meta-llama/Llama-2-7b-chat-hf...\n",
      "Loading model: meta-llama/Llama-2-7b-chat-hf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06eb82e491ec4563b0eb018c8a611bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading tokenizer for {model_id_1}...\")\n",
    "# The tokenizer turns our text prompt into numbers the model can understand.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id_1)\n",
    "\n",
    "print(f\"Loading model: {model_id_1}...\")\n",
    "# This downloads the model weights to your environment.\n",
    "# torch_dtype=torch.bfloat16 uses half-precision floats to save memory.\n",
    "# device_map=\"auto\" automatically puts the model on the GPU if available.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_1,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"{model_id_1} model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c227bd7e903141e0b901d6b6c368c9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                             prompt  \\\n",
      "0  6732655  3428081  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "1  1368762  1148749  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "2  8319432  9214800  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "3  6459722  7565469  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "4  5892625  9415651  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "5  8342682  1024647  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "6  8716638  1302271  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "7  7566899  2580901  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "8  8768610  5873151  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "9  3697407  4804185  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "\n",
      "     answer  response  correct  \n",
      "0  10160736  10154439    False  \n",
      "1   2517511   2517511     True  \n",
      "2  17534232  17533280    False  \n",
      "3  14025191   7322295    False  \n",
      "4  15308276  10308166    False  \n",
      "5   9367329   9367129    False  \n",
      "6  10018909  96250003    False  \n",
      "7  10147800   9647800    False  \n",
      "8  14641761  14640361    False  \n",
      "9   8501592   8502577    False  \n",
      "{'res': -1.5610148253968255, 'acc': 0.1, 'mae': 9844393.4, 'prompt_length': 63}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "added_prompt = ('Question: What is 3+7?\\nAnswer: 10\\n Question: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "prompt_config = {'max_tokens': 8,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2a**.\n",
    "* How does the performance change with the baseline in-context learning prompt? (compare with \"Example: Zero-shot 7-digit addition\" in Q1)\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: With the baseline in-context learning prompt, accuracy dropped to 10% and mae was approx 9.84 million. Compared to zero-shot 7-digit additions with maximum 8 tokens with the same decoding parameters, in-context learning performances drastically decreases.\n",
    "\n",
    "The single example is a 1-digit addition, which has no carries and doesn’t match with the 7 digit addition. The model learns the format Question-Answer but doesn’t learn the algorithmic reasoning for 7-digits addition. Limit of 8 tokens result in incomplete and truncated answer if the model produces extra token such as space, commas, or newline. The parameters values introduces higher stochasticity and no stop sequence makes the model to keep generating unnecessary tokens. Also, 1-digit addition example teaches the model that answers might be short, single-digit numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove the prior on output length and re-evaluate the performance of our baseline one-shot learning prompt. We need to modify our post processing function to extract the answer from the output sequence. In this case, it is the number in the first line that starts with \"Answer: \"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2b**.\n",
    "* Modify the post processing function\n",
    "* How does the performance change when we relax the output length constraint? (compare with Q2a)\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The post-processing function extracts the final answer from the model’s output. It splits the output at the Answer:, isolates the first line, cleans by removing commas, underscores, and extra spaces. Identifies and extracts numeric patterns, first look 8-digit numbers and then 7-digit numbers. If an 8-digit number is found, the last occurrence is returned, or else the function checks for a 7-digit number.\n",
    "\n",
    "Accuracy dropped from 10% to 0%, mae decreased from 9,844,393 to 5,304,260 when max tokens was increased.\n",
    "\n",
    "With higher max tokens, the model produces explanations, intermediate sums, commas/spaces, multiple numbers, adds extra texts and possibly prints answer in next line. This breaks the modified post processing function, and returns wrong values. This reduces accuracy and increases mae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your updated post processing function here\n",
    "def your_post_processing(output_string: str) -> int:\n",
    "    after = output_string.split(\"Answer:\", 1)[-1]\n",
    "    first_line = after.splitlines()[0] if after else \"\"\n",
    "    cleaned = first_line.replace(\",\", \"\").replace(\"_\", \" \").strip()\n",
    "\n",
    "    def _safe_int(s):\n",
    "        try:\n",
    "            return int(s)\n",
    "        except Exception:\n",
    "            return 0\n",
    "\n",
    "    m8 = re.findall(r\"(?<!\\d)(\\d{8})(?!\\d)\", cleaned)\n",
    "    if m8:\n",
    "        return _safe_int(m8[-1]) \n",
    "    m7 = re.findall(r\"(?<!\\d)(\\d{7})(?!\\d)\", cleaned)\n",
    "    if m7:\n",
    "        return _safe_int(m7[-1]) \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67cafdc15c29407b92d5c8be61544487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                             prompt  \\\n",
      "0  6732655  3428081  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "1  1368762  1148749  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "2  8319432  9214800  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "3  6459722  7565469  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "4  5892625  9415651  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "5  8342682  1024647  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "6  8716638  1302271  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "7  7566899  2580901  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "8  8768610  5873151  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "9  3697407  4804185  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "\n",
      "     answer  response  correct  \n",
      "0  10160736         0    False  \n",
      "1   2517511         0    False  \n",
      "2  17534232         0    False  \n",
      "3  14025191   7322188    False  \n",
      "4  15308276         0    False  \n",
      "5   9367329   9367129    False  \n",
      "6  10018909  10334999    False  \n",
      "7  10147800   9647800    False  \n",
      "8  14641761  14640211    False  \n",
      "9   8501592   8502592    False  \n",
      "{'res': -0.0, 'acc': 0.0, 'mae': 5304259.8, 'prompt_length': 63}\n"
     ]
    }
   ],
   "source": [
    "prompt_config['max_tokens'] = 50 # changed from 8, assuming we don't know the output length\n",
    "                \n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2c.** Let's change our one-shot learning example to something more \"in-distribution\". Previously we were using 1-digit addition as an example. Let's change it to 7-digit addition (1234567+1234567=2469134). \n",
    "* Evaluate the performance with max_tokens = 8.\n",
    "* Evaluate the performance with max_tokens = 50.\n",
    "* How does the performance change from 1-digit example to 7-digit example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: With the in-distribution one-shot prompt and 7-digit addition, the model’s performance improves compared to the 1-digit addition as example.\n",
    "Results\n",
    "– res: -0.204\n",
    "– acc: 0.2 or 20%\n",
    "– mae: 816,560.1\n",
    "– prompt length: 79\n",
    "7-digit addition example makes the in-distribution one-shot prompt allows the model to learn the addition pattern much better, it knows that output should have seven or eight digits. The accuracy doubled to 20%, and mae decreased drastically from 9.84 million to 0.82 million. It shows that the predictions are numerically closer to the actual results. The results show that in-distribution one-shot prompt improves performance even with few context length.\n",
    "\n",
    "With the in-distribution one-shot prompt and max tokens 50, the model’s performance decreases drastically compared to max tokens 8.\n",
    "Results\n",
    "– res: −0.0\n",
    "– acc: 0.0 or 0%\n",
    "– mae: 3.26 × 1031\n",
    "– prompt length: 79\n",
    "With max tokens 50 increased from 8 to 50 tokens, the model continues generating additional tokens beyond the correct numeric output instead of clean numerical answers which are mostly concatenations of multiple numbers rather than a valid sum. Local token prediction noise cascades under stochastic decoding. It causes mae to increase drastically and the accuracy to drop to zero.\n",
    "\n",
    "When the one-shot example was changed from a 1-digit addition to a 7-digit addition, the model’s performance improved under the same parameter values max tokens 8. Accuracy increased from 10% to 20%, and mae dropped drastically from about 9.84 million to roughly 0.82 million, and the predictions became numerically closer to the actual sum. This happened because the new example matched the distribution of the inputs, allows the model learn 7 digits addition much better. However, with max tokens 50, the model’s performed poorly. It increased mae to an extreme value to 3.26 × 1031 and reduced accuracy to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13a6a95f9314104a1b144518bbde852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                             prompt  \\\n",
      "0  1254878  2118550  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "1  7035620  6824705  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "2  6538466  4453098  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "3  9974889  9827518  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "4  7169878  6854133  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "5  7196020  4500293  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "6  2215869  7493395  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "7  5728189  3792177  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "8  5372518  9005390  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "9  9406391  4220157  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "\n",
      "     answer  response  correct  \n",
      "0   3373428   3373428     True  \n",
      "1  13860325  13859925    False  \n",
      "2  10991564  10983464    False  \n",
      "3  19802407  19802497    False  \n",
      "4  14024011  13964000    False  \n",
      "5  11696313  11696213    False  \n",
      "6   9709264   9612864    False  \n",
      "7   9520366   9520366     True  \n",
      "8  14377908  14377408    False  \n",
      "9  13626548   5626548    False  \n",
      "{'res': -0.20419243037974683, 'acc': 0.2, 'mae': 816560.1, 'prompt_length': 79}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 8 \n",
    "added_prompt = ('Question: What is 1234567+1234567?\\nAnswer: 2469134\\nQuestion: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46ffc21a9a049fea76d3a981db9d2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                             prompt  \\\n",
      "0  6143768  3896825  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "1  6348700  4041201  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "2  4524571  9012469  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "3  3044419  6608684  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "4  1756139  8493797  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "5  8083884  3154325  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "6  8888358  1527113  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "7  4025054  2352516  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "8  5053054  8166918  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "9  3075780  1468192  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "\n",
      "     answer                          response  correct  \n",
      "0  10040593  10034513987654323456781221812175    False  \n",
      "1  10389901  10389301987654376543217320973456    False  \n",
      "2  13537040  54360457890167234567810276445345    False  \n",
      "3   9653103  96490045678901853421014206310987    False  \n",
      "4  10249936  10257176234567893456789579246789    False  \n",
      "5  11238209  11237579987654323456781220821165    False  \n",
      "6  10415471  10459491987654323456781220220177    False  \n",
      "7   6377570  63775609837265678341551000752367    False  \n",
      "8  13219972  13120466987654323456781220221176    False  \n",
      "9   4543972  45440729876541234561009010234567    False  \n",
      "{'res': -0.0, 'acc': 0.0, 'mae': 3.255653744557156e+31, 'prompt_length': 79}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 50 \n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2d.** Let's look at a specific example with large absolute error. \n",
    "* Run the cell at least 5 times. Does the error change each time? Why?\n",
    "* Can you think of a prompt to reduce the error?\n",
    "* Why do you think it would work?\n",
    "* Does it work in practice? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: With each run, the error changes drastically, mean absolute error ranges from (1.01 × 1031) to (1.01 × 1032). This happens even though the prompt and inputs are identical. The randomness arises from sampling by temperature, top k, and top p. The model does not produce deterministic outputs and the responses vary widely. The model produces long concatenated sequences, increases the integer values, and produces large mae values. The error fluctuates because small differences in the model’s random sampling causes different numeric outputs once token generation begins, and the post-processing interprets those long texts as enormous numbers.\n",
    "\n",
    "You’re a calculator. Answer the question with final result only. Result should be in digits only, no spaces, commas, or text. Don’t give explanation of the answer. Don’t echo the question. Follow these rules strictly: Add digits from right to left. If a carry is 10 or more, add it to the next digit. If there is only a carry left, prepend it.\n",
    "\n",
    "The prompt works because it constrains both the model’s behavior and the decoding process. By instructing the model to only write the final number and not add extra text or questions, it doesn’t allow the model to keep generating text after the answer. And it makes the post processing eaier to extract the correct number and parse it.\n",
    "\n",
    "In practice, the prompt reduces large errors and answer will be off by only one or two digits. However, it does not guarantee perfect accuracy for multi-digit addition because the model is performing pattern completion, not algorithmic computation. While the constraints improve format consistency and fewer text generation, the model can still miscalculate due to limited numerical reasoning capability. It is trained in-distribution only on seven digits addition. Thus, the prompt makes formatting better and decreases large mae values, but it doesn’t solve the numeric reasoning limitations of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5182507c7964fdc810483d3a675fc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Prompt ************\n",
      "Question: What is 1234567+1234567?\n",
      "Answer: 2469134\n",
      "Question: What is 9090909+1010101?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "101010091\n",
      "Question: What is 12345678+12345678?\n",
      "Answer: 24691356\n",
      "Question: What is\n",
      "\n",
      "************ Final Output ************\n",
      "101010091123456781234567824691356\n",
      "     int_a    int_b                                             prompt  \\\n",
      "0  9090909  1010101  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "\n",
      "     answer                           response  correct  \n",
      "0  10101010  101010091123456781234567824691356    False  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'res': -0.0, 'acc': 0.0, 'mae': 1.0101009112345678e+32, 'prompt_length': 79}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    fixed_pairs=[(9090909,1010101)], \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    model_obj=model, \n",
    "    tokenizer_obj=tokenizer, \n",
    "    debug=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
